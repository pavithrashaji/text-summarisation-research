{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavithrashaji/text-summarisation-research/blob/main/Graph_Based_Summarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJIqiAqY6moi",
        "outputId": "fb475e8e-19a1-4a72-d77f-c64197f2d0a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9EabMlz63gD",
        "outputId": "068182ba-ae99-4c48-d1ad-a9fe352c34b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=pd.read_csv('/content/drive/MyDrive/DUC 2002 E/Input Files/DUC 2002 Extractive.csv')"
      ],
      "metadata": {
        "id": "e-tM4syE6u_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretext=pd.read_csv('/content/drive/MyDrive/DUC 2002 E/Input Files/NSW_S.csv')"
      ],
      "metadata": {
        "id": "7Bve3EWV6zAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "#Form a dictionary of words\n",
        "def preprocess(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = [word_tokenize(sentence) for sentence in sentences]\n",
        "    words = [word for sentence in words for word in sentence]\n",
        "    return words\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "#Build the graph\n",
        "def build_graph(words):\n",
        "    model = api.load('word2vec-google-news-300')\n",
        "\n",
        "    graph = nx.Graph()\n",
        "    for i, word in enumerate(words):\n",
        "        for j in range(i + 1, len(words)):\n",
        "            if word != words[j]:\n",
        "                if word in model.key_to_index and words[j] in model.key_to_index:\n",
        "                    similarity = np.dot(model.get_vector(word), model.get_vector(words[j])) / (np.linalg.norm(model.get_vector(word)) * np.linalg.norm(model.get_vector(words[j])))\n",
        "                    if similarity > 0:\n",
        "                        graph.add_edge(word, words[j], weight=similarity)\n",
        "\n",
        "    return graph\n",
        "\n",
        "#Summarize Module\n",
        "def summarize(text,pretext, num_sentences):\n",
        "    words = preprocess(pretext)\n",
        "    graph = build_graph(words)\n",
        "\n",
        "    #Compute scores of each word\n",
        "    scores = nx.pagerank(graph)\n",
        "\n",
        "    #Rank the sentences in descending order\n",
        "    ranked_sentences = sorted(((scores[word], sentence) for i, sentence in enumerate(sent_tokenize(text)) for word in word_tokenize(sentence.lower()) if word in scores), reverse=True)\n",
        "\n",
        "    #Extract the required sentences\n",
        "    summary=\"\"\n",
        "    count=0\n",
        "    for score,sentence in ranked_sentences:\n",
        "      if sentence not in summary and \"---\" not in sentence:\n",
        "        summary=summary+\" \"+sentence\n",
        "        count=count+1\n",
        "      if count==num_sentences:\n",
        "        break\n",
        "\n",
        "\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "H8re-dhrWmar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarize(text.iloc[0][2],pretext.iloc[0][2],num_sentences=15))"
      ],
      "metadata": {
        "id": "j0M9zzma1Qzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,16):\n",
        "  print(\"\\nSummary:\")\n",
        "  print(i)\n",
        "  print(summarize(text.iloc[i][2],pretext.iloc[i][2],num_sentences=15))\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "QtpWyKUS1QiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(16,31):\n",
        "  print(\"\\nSummary:\")\n",
        "  print(i)\n",
        "  print(summarize(text.iloc[i][2],pretext.iloc[i][2],num_sentences=15))\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "72LN7SUb1ZAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(31,46):\n",
        "  print(\"\\nSummary:\")\n",
        "  print(i)\n",
        "  print(summarize(text.iloc[i][2],pretext.iloc[i][2],num_sentences=15))\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "-xzHBrx-1aAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(46,59):\n",
        "  print(\"\\nSummary:\")\n",
        "  print(i)\n",
        "  print(summarize(text.iloc[i][2],pretext.iloc[i][2],num_sentences=15))\n",
        "  print(\"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "h-EAz_OSbx_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}